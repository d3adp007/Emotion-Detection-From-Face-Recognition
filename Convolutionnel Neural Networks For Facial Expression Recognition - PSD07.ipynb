{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65092b8d2d46a752c508ac3431b83298b2450702"
   },
   "source": [
    "# Convolutionnel Neural Networks For Facial Expression Recognition \n",
    "****\n",
    "Emotion detection from facial expression is one of the most active research fields, and plays a huge part in today’s technology. It can be implemented using machine learning algorithms, although these can’t provide  a hundred percent accurate solution since facial expression are not always the same and they depend on the person, the brightness, the position, and so on. This Notebook, presents an implementation of a deep learning algorithm for emotion detection using Convolutional Neural Network after some pre-processing steps to prepare our data uing OpenCV. Our choice of using CNN for this matter is based on the fact that this algorithm performs better than other solutions. Also, to conduct this experiment we have used a dataset which is a mix of other datasets like JAFFE and that was provided by Kaggle in the context of a competition.\n",
    "\n",
    "****\n",
    "This work was made by:\n",
    "    * Nasr Abdelhamid  \n",
    "                        abdelnasr7@gmail.com\n",
    "    * Omar Harchich \n",
    "                        omar.harchich@gmail.com\n",
    "Supervised by:\n",
    "    * Professor Elhannani Assmaa.\n",
    "    * Mrs Fatima Zahra Salmam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "124045eb12d92ed4c73d91e7855757198ad99a24"
   },
   "source": [
    "*****\n",
    "Our proposed method is divided to the following steps:\n",
    "    1. Data preparation\n",
    "    2. Image Processing\n",
    "    3. Build a COnvolutionnel Neural Networks Model\n",
    "    4. Test the model on another Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f73fd56e0a5334940f17b9762bbae5736777b963"
   },
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0952e7786abb6812ebf92d22eb3b48db39dba9a4"
   },
   "source": [
    "At first, we need to analyse our dataset in order to understand the features of images.\n",
    "*****\n",
    "Now we start by loading our csv file that contains both image filename and its corresponding emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('../input/dataset/dataset/data/legend.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf2204b3a35d982c2cf15789e3ef984d47c60cad"
   },
   "source": [
    "We display the loaded csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "251743c8dbf8ff52ea2a7ad5bb103ab3bf2c18e4"
   },
   "outputs": [],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2f5cad17e814e16b2a47342a6e4f05d9aaeb279"
   },
   "source": [
    "When checking the csv file, we noticed that some images have a corresponding emotion in lower case whereas others in upper case as shown in the execution below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c1288f883078dc82085f6662483988db73aed29"
   },
   "outputs": [],
   "source": [
    "train_csv.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8cc95cacb10cac98b0299fae553604e481f1dfc"
   },
   "source": [
    "That said, before moving forward, we need to normalize the emotions in the csv file. We will thus convert all emotions into lower case :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f7a89d5822dc981b92d08f56d3083d032d7463f"
   },
   "outputs": [],
   "source": [
    "train_csv['emotion'] = train_csv['emotion'].str.lower()\n",
    "train_csv.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ee58382b78b258797fac96b6dbf47dd638f20d8"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_csv['emotion'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "82723de79d93966ad3e6eb845b88d8807dc40fea"
   },
   "source": [
    "As we can see in the plot above, the contempt emotion doesn't have much records in the dataset. Therefore, it will not be as usefull as the others. That's why we will remove it by adding its images into the angry category : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f39fac7239dbfd4ead45b8980d03f3a15026bbd2"
   },
   "outputs": [],
   "source": [
    "train_csv.replace(\"contempt\", \"anger\", inplace=True)\n",
    "train_csv.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d433561d45a3d59681ad813245a46ed31235c89"
   },
   "outputs": [],
   "source": [
    "train_csv['emotion'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6294e498c7b131f833700620f4c10c79ae8f030f"
   },
   "source": [
    "After normalizing the emotions, we need to convert them into numeric labels in order to use them during the training process.\n",
    "For that we will use the map() function and store the result in a new variable called mapping_emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b354c51eddf2719f53e6af11266eb66b2065afc2"
   },
   "outputs": [],
   "source": [
    "mapping_emotion = {'anger': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, 'neutral': 6, 'sadness': 4, 'surprise': 5}\n",
    "train_csv['label'] = train_csv['emotion'].map(mapping_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5b8fc833b8f1407cc63bfe3362d747bff0ee84a"
   },
   "outputs": [],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "621dd24afd9196f611e97e4221a0a647d7eac3b1"
   },
   "source": [
    "# 2. Image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad46d9c02b0df157d6332ef62539077467dd4397"
   },
   "source": [
    "Now that we have our dataset normalized, we can proceed with the processing step. \n",
    "The training set is a collection of images where some are colored and others are converted to gray scale. The same is valid for the test set that also contains both colored and grayscaled images. Thus, we need to perform a grayscale conversion on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65ba562217fd6db368d54d1b66c89b91eb7a1b65"
   },
   "source": [
    "Let's first have a look at the training set. we will import the necessary libraries and then display a sample image of the training dataset that contains only grayscaled images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d10eaf47b36c5d8b442c662d2f563d9b9ff5009"
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "# Load image \n",
    "img_1 = cv.imread('../input/dataset/dataset/images/Aaron_Eckhart_0001.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19a36397f565889c4c538fc99fbb7840ca396f69"
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_1,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "30a0cc2625ed6b3299238b51885b4b204bf1862b"
   },
   "source": [
    "Colored images contain some information that is considered as noise in the image processing work. That's why we need to convert them to gray scale.\n",
    "\n",
    "Down below is the colored image after being converted to gary scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d41191d7c6db5e4eead1d5a08ad654d8f88acf0"
   },
   "outputs": [],
   "source": [
    "img_colored = cv.imread('../input/dataset/dataset/test/Katrinakaif_32.jpg')\n",
    "#plt.imshow(cv2.cvtColor(img_colored, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "gray_image = cv.cvtColor(img_colored, cv.COLOR_BGR2GRAY) \n",
    "#plt.imshow(gray_image,cmap='gray')\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img_colored, cv.COLOR_BGR2RGB))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(gray_image,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54e10aeaaaeab0adfc8390ec7df4a43eb5d79d89"
   },
   "source": [
    "Colors are not the only obstacle that we need to avoid, sometimes images contains other types of noise like the background information and so on. That's why we also need to perform cropping on images in order to keep only the regions of interest.\n",
    "\n",
    "In our case, we want to have images of size (48,48). But before cropping them we need to make sure that we keep the face region in the cropped image. That's why we need to proceed by detecting the face first, and then crop the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "be4f47149dbf105eba157fb6c784ef3b8265d193"
   },
   "outputs": [],
   "source": [
    "face_cascade = cv.CascadeClassifier('../input/haarcascade/haarcascade_frontalface_default.xml')\n",
    "height, width = gray_image.shape[:2]\n",
    "face = face_cascade.detectMultiScale(gray_image, 1.3, 1)\n",
    "if isinstance(face, tuple):\n",
    "    resized_image = cv.resize(gray_image, (48,48))\n",
    "        #cv.imwrite(trained+'/'+name,resized_image)\n",
    "elif isinstance(face, np.ndarray):\n",
    "    for (x,y,w,h) in face:\n",
    "        if w * h < (height * width) / 3:\n",
    "            resized_image = cv.resize(gray_image, (48,48)) \n",
    "                #cv.imwrite(trained+'/'+name,resized_image)\n",
    "        else:\n",
    "            roi_gray = gray_image[y:y+h, x:x+w]\n",
    "            resized_image = cv.resize(roi_gray, (48,48))\n",
    "                #cv.imwrite(trained+'/'+name, resized_image)\n",
    "    #if not name in deleting:\n",
    "    #data1.append(img_to_array(resized_image))\n",
    "plt.imshow(resized_image,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73d1e7219ac171833e612f0dc1398eff9ea404db"
   },
   "source": [
    "Now that we have done the necessary processing on the sample images, let's do the same to the rest of the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de4e9948ceb85a1bb5de3176fd066103ddcb1cdf"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2 as cv\n",
    "import os\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f02394cc36b771108af2ae8d8a12639cab99ff89"
   },
   "outputs": [],
   "source": [
    "trained = 'trainedimages'\n",
    "os.mkdir(trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73ab220270f57bb9ea88c74d89a20437b63f9abb"
   },
   "outputs": [],
   "source": [
    "face_cascade = cv.CascadeClassifier('../input/haarcascade/haarcascade_frontalface_default.xml')\n",
    "image_train = '../input/dataset/dataset/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1609356900307e8287ad2ad66e369f1d2417f213"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "663271ad971de46256d366adeab34b5bc9e24b6e"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for img in glob.glob(image_train+\"/*.jpg\"):\n",
    "    image = cv.imread(img)\n",
    "    name = img.split('/')[-1]\n",
    "    \n",
    "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY) # convert to greyscale\n",
    "    height, width = image.shape[:2]\n",
    "    faces = face_cascade.detectMultiScale(gray_image, 1.3, 1)\n",
    "    if isinstance(faces, tuple):\n",
    "        resized_image = cv.resize(gray_image, (48,48))\n",
    "        cv.imwrite(trained+'/'+name,resized_image)\n",
    "    \n",
    "    elif isinstance(faces, np.ndarray):\n",
    "        \n",
    "        for (x,y,w,h) in faces:\n",
    "            if w * h < (height * width) / 3:\n",
    "                resized_image = cv.resize(gray_image, (48,48)) \n",
    "                cv.imwrite(trained+'/'+name,resized_image)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                roi_gray = gray_image[y:y+h, x:x+w]\n",
    "                resized_image = cv.resize(roi_gray, (48,48))\n",
    "                cv.imwrite(trained+'/'+name, resized_image)\n",
    "    \n",
    "    \n",
    "    data.append(img_to_array(resized_image))\n",
    "    label = int(train_csv[ train_csv['image'] == name][['label']].values)    \n",
    "    labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "921ccd6f01f975a228959820636a72ec6bd1ac72"
   },
   "source": [
    "The following code displays the resulting images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "602944fa1991098fe60145b110909ec33b28b9ad"
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "import glob\n",
    "plt.figure(0, figsize=(12,6))\n",
    "for img in glob.glob(trained+'/*.jpg'):\n",
    "    img = cv.imread(img)\n",
    "    plt.subplot(4,4,i)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    i = i + 1\n",
    "    if i == 17:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "426defbe5de9c7c5e27b39345d5ca83c9053b21e"
   },
   "outputs": [],
   "source": [
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "print(\"[INFO] data matrix: {:.2f}MB\".format(data.nbytes / (1024 * 1000.0)))\n",
    "print(data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de640cb4b2e8c71146633908fc13fc9f95d79e51"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d9b44b62a1e5164aa1e921981d4e2a2a12bbf73"
   },
   "source": [
    "# 3. Build a Convolutionnel Neural Networks Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3fa49247bbb960e6514c2b41f35cb8a398e74488"
   },
   "source": [
    "In this part, we will build and train our model.\n",
    "\n",
    "Let's import the necessary libraries first :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5f74be4f6026f3d0d145988a70b011b3a5843ca"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3dceca3ffa6087f51426f221bb5cd6f933433d72"
   },
   "source": [
    "With deep learning, or any machine learning for that matter, a common practice is to make a training and testing split where we create an 70/30 random split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08f2f04149ca948c39144bc0e467a2d4d8620ee4"
   },
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits using 70% of\n",
    "# the data for training and the remaining 30% for testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "(trainX, valX, trainY, valY) = train_test_split(data,labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "288e0e72bd144f8abd0376f6fbb9722b34d71960"
   },
   "source": [
    "The neural network will receive as input a 48x48 grayscale image and then output the confidence of each expression.\n",
    "\n",
    "The network architecture comprises 5 convolutional layers, 3 subsampling layers and one fully connected layer. The first layer of the CNN is a convolution layer, that applies a convolution kernel of 3 ×3 and outputs 64 images of 48 x 48 pixels. This layer is followed by a subsampling layer that uses max-pooling (with kernel size 3 ×3) to reduce the image to the third of its size. The second and third convolutional layers, which output 64 images of 16 ×16 pixels, followed by a sub-sampling layer with kernel size 2 ×2. The fourth and fifth convolutional layers, output 128 images of size 8 ×8 pixels, and uses max pooling with kernel 2 ×2. The outputs are given to a fully connected hidden layer that has 1024 neurons. Finally, the network has six or seven output nodes (one for each expression that outputs their confidence level) that are fully connected to the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45fb9983f70c77158a37a1c617b5099b0e20e1af"
   },
   "outputs": [],
   "source": [
    "def buildModel(width, height, depth, classes):\n",
    "\t\t# initialize the model along with the input shape to be\n",
    "\t\t# \"channels last\" and the channels dimension itself\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# CONV => RELU => POOL\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# (CONV => RELU) * 2 => POOL\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# (CONV => RELU) * 2 => POOL\n",
    "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(1024))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tmodel.add(Dense(classes))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ffea3bc9c1e760ec3f93dfa8d9e5a596bf6f825d"
   },
   "source": [
    "For the training part, we will train our model for a hundred epochs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "771825ca6980a3ad9c3dcd5e17ce298a748bb484"
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs to train for, initial learning rate,\n",
    "# batch size, and image dimensions\n",
    "EPOCHS = 100\n",
    "INIT_LR = 1e-3\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba2b2e03d432921219778a2b04e1ecd736db4b40"
   },
   "outputs": [],
   "source": [
    "model = buildModel(width=48, height=48,depth=1, classes=len(lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "621f465058bc41b972a6a2052c60fc178f9af846"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa8bfd828039f0522c1db8f79c9107ed0baccc01"
   },
   "source": [
    "* EPOCHS:  The total number of epochs we will be training our network for (i.e., how many times our network “sees” each training example and learns patterns from it).\n",
    "\n",
    "* INIT_LR:  The initial learning rate — a value of 1e-3 is the default value for the Adam optimizer, the optimizer we will be using to train the network.\n",
    "\n",
    "* BS:  We will be passing batches of images into our network for training. There are multiple batches per epoch. The BS  value controls the batch size.\n",
    "\n",
    "* We’re going to use the Adam  optimizer with learning rate decay and then compile  our model  with categorical cross-entropy since we have > 2 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e19285f481a7f60b253339825134627e98e02449"
   },
   "source": [
    "Next, let’s create our image data augmentation object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0beb35e506be223d208038c62b47f095d10d7e6e"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "\thorizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d387aada491fb78c0800b7b55c5aeed439b7127e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "H = model.fit_generator(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tvalidation_data=(valX, valY),\n",
    "\tsteps_per_epoch=len(trainX) // BS,\n",
    "\tepochs=EPOCHS, verbose=1)\n",
    "print('[INFO] Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bb547a9f49229802a889a7fd1b3491b8c8cc70c"
   },
   "source": [
    "Once our Keras CNN has finished training, we’ll need to save both the model and label binarizer as we’ll need to load them from disk when we test the network on images outside of our training/testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "424db398be3f8cb2b16a6b9c547a6c439d57de42"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19068cad216e2c289c42ad16428e2324a22f4507"
   },
   "outputs": [],
   "source": [
    "model.save('emotion.model')\n",
    "f = open(\"lb.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(lb))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de953ef4ade4a2b3a6f6f7fca6bbae87b9725d37"
   },
   "source": [
    "Finally, we can plot our training and loss accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "454a9b1c5d4aabf83965a10f4822db37c645dfe4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(H.history['loss'], color='b', label='Training Loss')\n",
    "plt.plot(H.history['val_loss'], color='r', label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(H.history['acc'], color='b', label='Training Accuracy')\n",
    "plt.plot(H.history['val_acc'], color='r', label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a591b15e96161836164baa79fbdc0420fdb610a9"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('../input/modeltrain/emotion.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db55ca95820f3c88a56971d335a1eb6a78cf5997"
   },
   "source": [
    "# 4. Test the model on another Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f62bc8c63db3cd2b30aef72eb33610af8752a1c"
   },
   "source": [
    "Now we are ready to make a prediction with a set of images. These should be processed with the same method implemented in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d471d5eee164913cfd4aa771a9f3b7eb2da68f9f"
   },
   "outputs": [],
   "source": [
    "image_test = '../input/dataset/dataset/test'\n",
    "os.mkdir('test_pretraitement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f357e79f584c7d256ba6a5a903854c736b39563"
   },
   "outputs": [],
   "source": [
    "test_pretraitement = 'test_pretraitement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f2668c337619032219a7364ff9f913af4a5e5f4"
   },
   "outputs": [],
   "source": [
    "data_test = {}\n",
    "labels_test = {}\n",
    "i = 0\n",
    "for img in glob.glob(image_test+\"/*.jpg\"):\n",
    "    image = cv.imread(img)\n",
    "    name = img.split('/')[-1]\n",
    "    \n",
    "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY) # convert to greyscale\n",
    "    height, width = image.shape[:2]\n",
    "    faces = face_cascade.detectMultiScale(gray_image, 1.3, 1)\n",
    "    if isinstance(faces, tuple):\n",
    "        resized_image = cv.resize(gray_image, (48, 48))\n",
    "        cv.imwrite(test_pretraitement+'/'+name,resized_image)\n",
    "    #print(faces)\n",
    "    elif isinstance(faces, np.ndarray):\n",
    "        for (x,y,w,h) in faces:\n",
    "            if w * h < (height * width) / 3:\n",
    "                resized_image = cv.resize(gray_image, (48, 48)) \n",
    "                cv.imwrite(test_pretraitement+'/'+name,resized_image)\n",
    "            else:\n",
    "                \n",
    "                #cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                roi_gray = gray_image[y:y+h, x:x+w]\n",
    "                #print(len(roi_gray))\n",
    "                resized_image = cv.resize(roi_gray, (48, 48))\n",
    "                cv.imwrite(test_pretraitement+'/'+name, resized_image)\n",
    "    image = resized_image.astype(\"float\") / 255.0\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    data_test[name] = image\n",
    "    #data.append(img_to_array(resized_image))\n",
    "    \n",
    "    #print(label, type(label), name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37ad4c421fc837f452e27141facddcb1896d6909"
   },
   "source": [
    "From there we can start implementing our script to classify images that are not part of our training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df04ca3bd51fa76dab33773483fc22653b5feb75"
   },
   "outputs": [],
   "source": [
    "data_predict = {}\n",
    "for key,value in data_test.items():\n",
    "    predict = model.predict(value)\n",
    "    idx = np.argmax(predict)\n",
    "    l= lb.classes_[idx]\n",
    "    data_predict[key] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9780dcee99d31c87fe73e8e721f96df11c61a919"
   },
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame(list(data_predict.items()),\n",
    "                      columns=['Image','Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2dfb71eef810b7a6739e35f32636e8a880e04a12"
   },
   "outputs": [],
   "source": [
    "mapping_emotion = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 6:'neutral', 4:'sadness', 5:'surprise'}\n",
    "final_data['Emotion'] = final_data['Emotion'].map(mapping_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19b8e348868bf0afa78577c5f9a3f90c0a528cce"
   },
   "outputs": [],
   "source": [
    "final_data.to_csv('submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3da8d15201a7c4b0a66cc801e0624c5764401ffe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
